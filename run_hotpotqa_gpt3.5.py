import os
import re
import random
import time
import openai
import json
import wikienv, wrappers
from tqdm import tqdm
from uncertainty_utils import *

openai.api_key = os.environ["OPENAI_API_KEY"]
mode = "standard" # choose from [standard, cot, react, uala]
uncertainty_estimation_method = "entropy" # choose from [min, avg, log_sum, norm, entropy]
oracle = True # whether to use oracle in uala
save_file_name = "outputs/chatgpt-hotpotqa-dev-standard.jsonl" # saved file name

# load pre-calculated uncertainty threshold based on calibration set
if uncertainty_estimation_method == "min":
    cal_uncertainty = cal_uncertainty_min
    uncertainty_threshold = 1.79
elif uncertainty_estimation_method == "avg":
    cal_uncertainty = cal_uncertainty_mean
    uncertainty_threshold = 1.79
elif uncertainty_estimation_method == "log_sum":
    cal_uncertainty = cal_uncertainty_log_sum
    uncertainty_threshold = 10.75
elif uncertainty_estimation_method == "norm":
    cal_uncertainty = cal_uncertainty_norm
    uncertainty_threshold = 5.38
elif uncertainty_estimation_method == "entropy":
    cal_uncertainty = cal_uncertainty_entropy
    uncertainty_threshold = 1.79

def llm(prompt, stop=["\n"], return_probs=False):
    response = openai.Completion.create(
      model="gpt-3.5-turbo-instruct",
      prompt=prompt,
      temperature=0,
      max_tokens=128,
      frequency_penalty=0.0,
      presence_penalty=0.0,
      stop=stop,
      logprobs=1
    )
    if return_probs:
        return response["choices"][0]
    else:
        return response["choices"][0]["text"]

env = wikienv.WikiEnv()
env = wrappers.HotPotQAWrapper(env, split="dev")
env = wrappers.LoggingWrapper(env)

def step(env, action):
    attempts = 0
    while attempts < 10:
        try:
            return env.step(action)
        except requests.exceptions.Timeout:
            attempts += 1

prompt_file = './prompts/prompts.json'
with open(prompt_file, 'r') as f:
    prompt_dict = json.load(f)

# standard prompt
hotpotqa_standard_examples = prompt_dict['hotpotqa_standard']
instruction_standard = """Answer the question:\n"""
hotpotqa_standard_prompt = instruction_standard + hotpotqa_standard_examples

# cot prompt
hotpotqa_cot_examples = prompt_dict['hotpotqa_cot']
instruction_cot = """Solve a question answering task. Your task is to generate Thought and Answer where a Thought can reason about the current situation by thinking step by step.
Here are some examples.
"""
hotpotqa_cot_prompt = instruction_cot + hotpotqa_cot_examples

# react prompt
hotpotqa_react_examples = prompt_dict['hotpotqa_react']
instruction_react = """Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be three types: 
(1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it will return some similar entities to search.
(2) Lookup[keyword], which returns the next sentence containing keyword in the current passage.
(3) Finish[answer], which returns the answer and finishes the task.
Here are some examples.
"""
hotpotqa_react_prompt = instruction_react + hotpotqa_react_examples


def standard(idx=None, prompt=hotpotqa_standard_prompt, to_print=True):
    question = env.reset(idx=idx)
    if to_print:
        print(idx, question)
    prompt += question + "\n"
    answer = llm(prompt + f"Answer:", stop=[f"\n"], return_probs=True)
    if to_print:
        print("Answer:", answer["text"])
    return answer

def cot(idx=None, prompt=hotpotqa_cot_prompt, to_print=True):
    question = env.reset(idx=idx)
    if to_print:
        print(idx, question)
    prompt += question + "\n"
    answer = llm(prompt + f"Thought:", stop=[f"\nQuestion:"], return_probs=True)
    if to_print:
        print("Thought:", answer["text"])
    return answer

def react(idx=None, prompt=hotpotqa_react_prompt, to_print=True):
    question = env.reset(idx=idx)
    if to_print:
        print(idx, question)
    prompt += question + "\n"
    n_calls, n_badcalls = 0, 0
    react_probs = []
    for i in range(1, 8):
        n_calls += 1
        thought_action_probs = llm(prompt + f"Thought {i}:", stop=[f"\nObservation {i}:"], return_probs=True)
        react_probs.append(thought_action_probs)
        thought_action = thought_action_probs["text"]
        try:
            thought, action = thought_action.strip().split(f"\nAction {i}: ")
        except:
            print('ohh...', thought_action)
            n_badcalls += 1
            n_calls += 1
            thought = thought_action.strip().split('\n')[0]
            action_probs = llm(prompt + f"Thought {i}: {thought}\nAction {i}:", stop=[f"\n"], return_probs=True)
            react_probs.append(action_probs)
            action = action_probs["text"].strip()
        obs, r, done, info = step(env, action[0].lower() + action[1:])
        obs = obs.replace('\\n', '')

        step_str = f"Thought {i}: {thought}\nAction {i}: {action}\nObservation {i}: {obs}\n"
        prompt += step_str
        if to_print:
            print(step_str)
        if done:
            break
    if not done:
        obs, r, done, info = step(env, "finish[]")

    if to_print:
        print(info, '\n')
    info.update({'n_calls': n_calls, 'n_badcalls': n_badcalls, 'traj': prompt})
    return info, react_probs


idxs = list(range(7405))
random.Random(233).shuffle(idxs)

evals = []
old_time = time.time()

num_tool_call_instance = 0
num_instance = 0
num_correct = 0
num_tool_calls = 0
num_backoff = 0
num_ask_human = 0

with open(save_file_name,"a") as output_file:
    for i in tqdm(idxs[:500]):
        question = env.reset(idx=i)
        gold_answer = env.data[i][1]
        num_instance += 1

        if mode == "standard":
            standard_output = standard(i, to_print=True)
            print('-----------')
            predicted_answer = standard_output["text"].strip()
            em = (wrappers.normalize_answer(predicted_answer) == wrappers.normalize_answer(gold_answer))
            f1 = wrappers.f1_score(wrappers.normalize_answer(predicted_answer), wrappers.normalize_answer(gold_answer))[0]
            standard_final_output = {"answer": predicted_answer, "gt_answer": gold_answer, "question_idx": i, "em": em, "f1": f1, "traj": question + '\nAnswer:' + standard_output["text"]}
            if standard_final_output["em"]:
                num_correct += 1
            output_file.write(json.dumps(standard_final_output, ensure_ascii=False) + '\n')

        elif mode == "cot":
            cot_output = cot(i, to_print=True)
            print('-----------')
            try:
                predicted_answer = cot_output["text"].split('Answer:')[1].strip()
            except:
                try:
                    predicted_answer = cot_output["text"].split("the answer is ")[1].strip()
                except:
                    predicted_answer = ""
            em = (wrappers.normalize_answer(predicted_answer) == wrappers.normalize_answer(gold_answer))
            f1 = wrappers.f1_score(wrappers.normalize_answer(predicted_answer), wrappers.normalize_answer(gold_answer))[0]
            cot_final_output = {"answer": predicted_answer, "gt_answer": gold_answer, "question_idx": i, "em": em, "f1": f1, "traj": question + '\nThought:' + cot_output["text"]}
            if cot_final_output["em"]:
                num_correct += 1
            output_file.write(json.dumps(cot_final_output, ensure_ascii=False) + '\n')

        elif mode == "react":
            info, _ = react(i, to_print=True)
            evals.append(info['em'])
            print(sum(evals), len(evals), sum(evals) / len(evals), (time.time() - old_time) / len(evals))
            print('-----------')
            info["traj"] = info["traj"].split(hotpotqa_react_prompt)[1].strip()
            num_tool_calls += info["n_calls"]
            if info["em"]:
                num_correct += 1
            output_file.write(json.dumps(info, ensure_ascii=False) + '\n')

        elif mode == "uala":
            print('-----------CoT-----------')
            cot_output = cot(i, to_print=True)
            try:
                predicted_answer = cot_output["text"].split('Answer:')[1].strip()
            except:
                try:
                    predicted_answer = cot_output["text"].split("the answer is ")[1].strip()
                except:
                    predicted_answer = ""

            em = (wrappers.normalize_answer(predicted_answer) == wrappers.normalize_answer(gold_answer))
            f1 = wrappers.f1_score(wrappers.normalize_answer(predicted_answer), wrappers.normalize_answer(gold_answer))[0]
            cot_final_output = {"steps": 2, "answer": predicted_answer, "gt_answer": gold_answer, "question_idx": i, "reward": em, "em": em, "f1": f1, "n_calls": 0, "n_badcalls": 0, "traj": question + '\nThought 1:' + cot_output["text"] + f'\nAction 1: MeasureUncertainty [{predicted_answer}]'}
            
            # extract answer token logprobs for hotpotqa
            try:
                idx = cot_output['logprobs']['tokens'].index("Answer")
                answer_probs = cot_output['logprobs']['token_logprobs'][idx+2:]
                if not answer_probs:
                    answer_probs = cot_output['logprobs']['token_logprobs']
            except:
                answer_probs = cot_output['logprobs']['token_logprobs']

            # calculate uncertainty
            uncertainty = cal_uncertainty(answer_probs, 5)

            # make tool use
            if uncertainty > uncertainty_threshold:
                print(f'-----------Answerâ€™s uncertainty {round(uncertainty,2)}, which falls outside the acceptable uncertainty threshold of {uncertainty_threshold}, I need to use an external tool to solve the question.-----------')
                num_tool_call_instance += 1
                print('-----------ReAct-----------')
                info, react_probs = react(i, to_print=True)
                predicted_react_answer = info["answer"]
                cot_final_output["traj"] += f"\nObservation 1: Answerâ€™s uncertainty is {round(uncertainty,2)}, which falls outside the acceptable threshold of {uncertainty_threshold}.\nThought 2: Based on the uncertainty, I need to use an external tool to solve the question.\nAction 2: Activate Tool.\n"
                
                def reformat(text):
                    return f"{text.group(1)} {int(text.group(2)) + 2}"

                react_traj = info["traj"].split(hotpotqa_react_prompt + question + "\n")[1].strip()
                reformat_react_traj = re.compile(r'(Thought|Observation|Action) (\d+)').sub(reformat, react_traj)

                last_index = int(re.findall(r'(Thought|Observation|Action)\s*(\d+)', reformat_react_traj)[-1][-1])

                info["traj"] = cot_final_output["traj"] + reformat_react_traj 
                info["steps"] += cot_final_output["steps"]
                num_tool_calls += info["n_calls"]

                if info["answer"]:
                    info["traj"] += f"\nThought {str(last_index+1)}: Let me check the uncertainty of the returned answer.\nAction {str(last_index+1)}: MeasureUncertainty [{predicted_react_answer}]"
                    info["steps"] += 1

                    # extract the answer token probs
                    for output in react_probs:
                        if " Finish" in output['text']:
                            try:
                                idx = output['logprobs']['tokens'].index(" answer")
                                idx_end = output['logprobs']['tokens'].index(".\n")
                                answer_probs = output['logprobs']['token_logprobs'][idx+2:idx_end]     
                            except:
                                idx = output['logprobs']['tokens'].index(" Finish")
                                answer_probs = output['logprobs']['token_logprobs'][idx+2:-1]
                            if not answer_probs:
                                answer_probs = [0.0]                               
                        else:
                            answer_probs = [0.0]

                    react_uncertainty = cal_uncertainty(answer_probs, 5)
                    if react_uncertainty > uncertainty_threshold:
                        info["steps"] += 1
                        if oracle:
                            print(f"-----------Answerâ€™s uncertainty is {round(react_uncertainty,2)}, which falls outside the acceptable threshold of {uncertainty_threshold}, ask a human for help.-----------")
                            info["traj"] += f"\nObservation {str(last_index+2)}: Answerâ€™s uncertainty is {round(react_uncertainty,2)}, which falls outside the acceptable threshold of {uncertainty_threshold}.\nThought {str(last_index+2)}: Based on the uncertainty, I need to ask a human for help.\nAction {str(last_index+2)}: Ask Human.\nObservation {str(last_index+2)}: {gold_answer}\nAnswer: {gold_answer}"
                            info["answer"] = gold_answer
                            info["reward"] = True
                            info["em"] = True
                            info["f1"] = 1.0
                            num_ask_human += 1
                        else:
                            print(f"-----------Answerâ€™s uncertainty is {round(react_uncertainty,2)}, which falls outside the acceptable threshold of {uncertainty_threshold}, for simplicity, answer is still kept.-----------")
                            info["traj"] += f"\nObservation {str(last_index+2)}: Answerâ€™s uncertainty is {round(react_uncertainty,2)}, which falls outside the acceptable threshold of {uncertainty_threshold}.\nThought {str(last_index+2)}: For simplicity, answer is still kept.\nAction {str(last_index+2)}: Keep Answer.\nAnswer: {predicted_react_answer}"
                            
                    else:
                        print(f"-----------Answerâ€™s uncertainty is {round(react_uncertainty,2)}, which falls within the acceptable threshold of {uncertainty_threshold}, answer is kept.-----------")
                        info["traj"] += f"\nObservation {str(last_index+2)}: Answerâ€™s uncertainty is {round(react_uncertainty,2)}, which falls within the acceptable threshold of {uncertainty_threshold}.\nThought {str(last_index+2)}: Based on the uncertainty, answer is kept.\nAction {str(last_index+2)}: Keep Answer.\nAnswer: {predicted_react_answer}"
                        info["steps"] += 1
                    
                else:
                    print("-----------Returned tool-use answer is invalid, I need to use the backoff answer.-----------")
                    use_backoff_traj = f"\nThought {str(last_index+1)}: Returned tool-use answer is invalid, I need to use the backoff answer.\nAction {str(last_index+1)}: Use Backoff Answer.\nAnswer: {predicted_answer}"
                    info["traj"] += use_backoff_traj
                    num_backoff += 1
                    info["answer"] = cot_final_output["answer"]
                    info["reward"] = cot_final_output["reward"]
                    info["em"] = cot_final_output["em"]
                    info["f1"] = cot_final_output["f1"]

                if info["em"]:
                    num_correct += 1
                output_file.write(json.dumps(info, ensure_ascii=False) + '\n')
            else:
                print(f'-----------Answerâ€™s uncertainty is {round(uncertainty,2)}, which falls within the acceptable threshold of {uncertainty_threshold}, answer is kept.-----------')
                cot_final_output["traj"] += f"\nObservation 1: Answerâ€™s uncertainty is {round(uncertainty,2)}, which falls within the acceptable threshold of {uncertainty_threshold}.\nThought 2: Based on the uncertainty, answer is kept.\nAction 2: Keep Answer.\nAnswer: {predicted_answer}"
                if cot_final_output["em"]:
                    num_correct += 1
                output_file.write(json.dumps(cot_final_output, ensure_ascii=False) + '\n')
