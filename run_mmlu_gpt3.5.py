import os
import re
import time
import openai
import json
import wikienv, wrappers
from tqdm import tqdm
from search import search
from uncertainty_utils import *

openai.api_key = os.environ["OPENAI_API_KEY"]
mode = "uala" # choose from [standard, cot, react, uala]
oracle = True # whether to use oracle in uala
save_file_name = "outputs/chatgpt-mmlu-test-uala-oracle.jsonl" # saved file name

# load pre-calculated uncertainty threshold based on calibration set
uncertainty_threshold = 0.31

def llm(prompt, stop=["\n"], return_probs=False):
    response = openai.Completion.create(
      model="gpt-3.5-turbo-instruct",
      prompt=prompt,
      temperature=0,
      max_tokens=128,
      frequency_penalty=0.0,
      presence_penalty=0.0,
      stop=stop,
      logprobs=1
    )
    if return_probs:
        return response["choices"][0]
    else:
        return response["choices"][0]["text"]

env = wikienv.WikiEnv()
env = wrappers.MMLUWrapper(env, split="test")
env = wrappers.LoggingWrapper(env)

prompt_file = './prompts/prompts.json'
with open(prompt_file, 'r') as f:
    prompt_dict = json.load(f)

# standard prompt
mmlu_standard_examples = prompt_dict['mmlu_standard']
instruction_standard = """Answer the question:\n"""
mmlu_standard_prompt = instruction_standard + mmlu_standard_examples

# cot prompt
mmlu_cot_examples = prompt_dict['mmlu_cot']
instruction_cot = """Solve a question answering task. Your task is to generate Thought and Answer where a Thought can reason about the current situation by thinking step by step.
Here are some examples.
"""
mmlu_cot_prompt = instruction_cot + mmlu_cot_examples

# react prompt
mmlu_react_examples = prompt_dict['mmlu_react_google']
instruction_react_google = """Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be two types: 
(1) search[question], which searches a question on Google and returns a short snippet containing the answer. Note that sometimes the snippet does not contain the answer, and some alternative search might be needed.
(2) finish[answer], which returns the answer and finishes the task. 
Here are some examples.
"""
mmlu_react_prompt = instruction_react_google + mmlu_react_examples

def standard(idx=None, prompt=mmlu_standard_prompt, to_print=True):
    question = env.reset(idx=idx)
    if to_print:
        print(idx, question)
    prompt += question + "\n"
    answer = llm(prompt + f"Answer:", stop=[f"\n"], return_probs=True)
    if to_print:
        print("Answer:", answer["text"])
    return answer

def cot(idx=None, prompt=mmlu_cot_prompt, to_print=True):
    question = env.reset(idx=idx)
    if to_print:
        print(idx, question)
    prompt += question + "\n"
    answer = llm(prompt + f"Thought:", stop=[f"\nQuestion:"], return_probs=True)
    if to_print:
        print("Thought:", answer["text"])
    return answer

def react(idx=None, prompt=mmlu_react_prompt, to_print=True):
    question = env.reset(idx=idx)
    gold_answer = env.data[idx][1]
    if to_print:
        print(idx, question)
    prompt += question + "\n"
    n_calls, n_badcalls = 0, 0
    steps = 0
    answer = ""
    react_probs = []
    for i in range(1, 8):
        n_calls += 1
        thought_action_probs = llm(prompt + f"Thought:", stop=[f"\nObservation:"], return_probs=True)
        thought_action = thought_action_probs["text"]
        react_probs.append(thought_action_probs)
        try:
            thought, action = thought_action.strip().split(f"\nAction: ")
        except:
            print('ohh...', thought_action)
            n_badcalls += 1
            n_calls += 1
            thought = thought_action.strip().split('\n')[0]
            action_probs = llm(prompt + f"Thought: {thought}\nAction:", stop=[f"\n"], return_probs=True)
            action = action_probs["text"].strip()
            react_probs.append(action_probs)

        if action.startswith("search[") and action.endswith("]"):
            search_query = action[len("search["):-1]
            obs = search(search_query)
            step_str = f"Thought: {thought}\nAction: {action}\nObservation: {obs}\n"
            if to_print:
                print(step_str)
            prompt += step_str
            steps = i
        elif action.startswith("finish[") and action.endswith("]"):
            answer = action[len("finish["):-1]
            step_str = f"Thought: {thought}\nAction: {action}\n"
            if to_print:
                print(step_str)
            prompt += step_str
            steps = i
            break
        else:
            print("Invalid action: {}".format(action))
            step_str = f"Thought: {thought}\nAction: finish[]\n"
            if to_print:
                print(step_str)
            prompt += step_str
            steps = i
            break

    em = (answer.lower() == gold_answer.lower())
    info = {'steps':steps, "answer": answer, "gt_answer": gold_answer, "question_idx": idx, "reward": em, "em": em, 'n_calls': n_calls, 'n_badcalls': n_badcalls, 'traj': prompt}
    return info, react_probs


evals = []
old_time = time.time()

num_tool_call_instance = 0
num_instance = 0
num_correct = 0
num_tool_calls = 0
num_backoff = 0
num_ask_human = 0

with open(save_file_name,"a") as output_file:
    for i in tqdm(range(len(env))):
        question = env.reset(idx=i)
        gold_answer = env.data[i][1]
        num_instance += 1

        if mode == "standard":
            standard_output = standard(i, to_print=True)
            print('-----------')
            predicted_answer = standard_output["text"].strip()
            em = (predicted_answer.lower() == gold_answer.lower())
            standard_final_output = {"answer": predicted_answer, "gt_answer": gold_answer, "question_idx": i, "em": em, "traj": question + '\nAnswer:' + standard_output["text"]}
            if standard_final_output["em"]:
                num_correct += 1
            output_file.write(json.dumps(standard_final_output, ensure_ascii=False) + '\n')

        elif mode == "cot":
            cot_output = cot(i, to_print=True)
            print('-----------')
            try:
                predicted_answer = cot_output["text"].split('Answer:')[1].strip()
            except:
                try:
                    predicted_answer = cot_output["text"].split("the answer is ")[1].strip()
                except:
                    predicted_answer = ""
            em = (predicted_answer.lower() == gold_answer.lower())
            cot_final_output = {"answer": predicted_answer, "gt_answer": gold_answer, "question_idx": i, "em": em, "traj": question + '\nThought:' + cot_output["text"]}
            if cot_final_output["em"]:
                num_correct += 1
            output_file.write(json.dumps(cot_final_output, ensure_ascii=False) + '\n')

        elif mode == "react":
            info, _ = react(i, to_print=True)
            evals.append(info['em'])
            print(sum(evals), len(evals), sum(evals) / len(evals), (time.time() - old_time) / len(evals))
            print('-----------')
            info["traj"] = info["traj"].split(mmlu_react_prompt)[1].strip()
            num_tool_calls += info["n_calls"]
            if info["em"]:
                num_correct += 1
            output_file.write(json.dumps(info, ensure_ascii=False) + '\n')

        elif mode == "uala":
            print('-----------Standard-----------')
            standard_output = standard(i, to_print=True)
            predicted_answer = standard_output["text"].strip()

            em = (predicted_answer.lower() == gold_answer.lower())
            standard_final_output = {"steps": 2, "answer": predicted_answer, "gt_answer": gold_answer, "question_idx": i, "reward": em, "em": em, "n_calls": 0, "n_badcalls": 0, "traj": question + '\nThought:' + standard_output["text"] + f'\nAction: MeasureUncertainty [{predicted_answer}]'}
            
            # extract answer token logprobs for mmlu
            answer_probs = standard_output['logprobs']['token_logprobs']

            # calculate uncertainty
            uncertainty = cal_uncertainty_single_token(answer_probs)

            # make tool use
            if uncertainty > uncertainty_threshold:
                print(f'-----------Answer’s uncertainty {round(uncertainty,2)}, which falls outside the acceptable uncertainty threshold of {uncertainty_threshold}, I need to use an external tool to solve the question.-----------')
                num_tool_call_instance += 1
                print('-----------ReAct-----------')
                info, react_probs = react(i, to_print=True)
                predicted_react_answer = info["answer"]
                standard_final_output["traj"] += f"\nObservation: Answer’s uncertainty is {round(uncertainty,2)}, which falls outside the acceptable threshold of {uncertainty_threshold}.\nThought: Based on the uncertainty, I need to use an external tool to solve the question.\nAction: Activate Tool.\n"
                
                def reformat(text):
                    return f"{text.group(1)} {int(text.group(2)) + 2}"

                react_traj = info["traj"].split(mmlu_react_prompt + question + "\n")[1].strip()

                info["traj"] = standard_final_output["traj"] + react_traj 
                info["steps"] += standard_final_output["steps"]
                num_tool_calls += info["n_calls"]

                if info["answer"] in ['A', 'B', 'C', 'D']:
                    info["traj"] += f"\nThought: Let me check the uncertainty of the returned answer.\nAction: MeasureUncertainty [{predicted_react_answer}]"
                    info["steps"] += 1

                    # extract the answer token probs
                    for output in react_probs:
                        if " finish" in output['text']:
                            answer = output['text'].split(" finish[")[1].split("]")[0]
                            try:
                                idx = output['logprobs']['tokens'].index(" answer")
                                answer_probs = output['logprobs']['token_logprobs'][idx+2:idx+3]     
                            except:
                                idx = output['logprobs']['tokens'].index(" finish")
                                answer_probs = output['logprobs']['token_logprobs'][idx+1:-1]
                            if not answer_probs:
                                answer_probs = [0.0]
                        else:
                            answer = ""
                            answer_probs = [0.0]

                    react_uncertainty = cal_uncertainty_single_token(answer_probs)
                    if react_uncertainty > uncertainty_threshold:
                        info["steps"] += 1
                        if oracle:
                            print(f"-----------Answer’s uncertainty is {round(react_uncertainty,2)}, which falls outside the acceptable threshold of {uncertainty_threshold}, ask a human for help.-----------")
                            info["traj"] += f"\nObservation: Answer’s uncertainty is {round(react_uncertainty,2)}, which falls outside the acceptable threshold of {uncertainty_threshold}.\nThought: Based on the uncertainty, I need to ask a human for help.\nAction: Ask Human.\nObservation: {gold_answer}\nAnswer: {gold_answer}"
                            info["answer"] = gold_answer
                            info["reward"] = True
                            info["em"] = True
                            num_ask_human += 1
                        else:
                            print(f"-----------Answer’s uncertainty is {round(react_uncertainty,2)}, which falls outside the acceptable threshold of {uncertainty_threshold}, for simplicity, answer is still kept.-----------")
                            info["traj"] += f"\nObservation: Answer’s uncertainty is {round(react_uncertainty,2)}, which falls outside the acceptable threshold of {uncertainty_threshold}.\nThought: For simplicity, answer is still kept.\nAction: Keep Answer.\nAnswer: {predicted_react_answer}"
                            
                    else:
                        print(f"-----------Answer’s uncertainty is {round(react_uncertainty,2)}, which falls within the acceptable threshold of {uncertainty_threshold}, answer is kept.-----------")
                        info["traj"] += f"\nObservation: Answer’s uncertainty is {round(react_uncertainty,2)}, which falls within the acceptable threshold of {uncertainty_threshold}.\nThought: Based on the uncertainty, answer is kept.\nAction: Keep Answer.\nAnswer: {predicted_react_answer}"
                        info["steps"] += 1
                    
                else:
                    print("-----------Returned tool-use answer is invalid, I need to use the backoff answer.-----------")
                    use_backoff_traj = f"\nThought: Returned tool-use answer is invalid, I need to use the backoff answer.\nAction: Use Backoff Answer.\nAnswer: {predicted_answer}"
                    info["traj"] += use_backoff_traj
                    num_backoff += 1
                    info["answer"] = standard_final_output["answer"]
                    info["reward"] = standard_final_output["reward"]
                    info["em"] = standard_final_output["em"]

                if info["em"]:
                    num_correct += 1
                output_file.write(json.dumps(info, ensure_ascii=False) + '\n')
            else:
                print(f'-----------Answer’s uncertainty is {round(uncertainty,2)}, which falls within the acceptable threshold of {uncertainty_threshold}, answer is kept.-----------')
                standard_final_output["traj"] += f"\nObservation 1: Answer’s uncertainty is {round(uncertainty,2)}, which falls within the acceptable threshold of {uncertainty_threshold}.\nThought: Based on the uncertainty, answer is kept.\nAction: Keep Answer.\nAnswer: {predicted_answer}"
                if standard_final_output["em"]:
                    num_correct += 1
                output_file.write(json.dumps(standard_final_output, ensure_ascii=False) + '\n')
