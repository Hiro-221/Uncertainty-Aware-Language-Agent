import os
import re
import time
import torch
import json
from tqdm import tqdm
from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig
from utils.prompter import Prompter
from peft import PeftModel
import wikienv, wrappers
from tqdm import tqdm
from search import search
from uncertainty_utils import *

base_model = "meta-llama/Llama-2-70b-hf"
load_in_4bit = False # set False to use 8-bit
mode = "uala" # choose from [standard, cot, react, uala]
oracle = True # whether to use oracle in uala
save_file_name = "outputs/llama2-mmlu-dev-uala-oracle.jsonl" # saved file name

# load pre-calculated uncertainty threshold based on calibration set
uncertainty_threshold = 0.43


tokenizer = LlamaTokenizer.from_pretrained(base_model)
prompter = Prompter("llama2")
if load_in_4bit:
    bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
    )
    model = LlamaForCausalLM.from_pretrained(
            base_model,
            quantization_config=bnb_config,
            device_map="auto",
        )
else:
    model = LlamaForCausalLM.from_pretrained(
            base_model,
            load_in_8bit=True,
            torch_dtype=torch.float16,
            device_map="auto",
        )

model.eval()

def llama2_prompt(
    instruction,
    input=None,
    temperature=0,
    top_p=1,
    num_beams=1,
    do_sample=False,
    max_new_tokens=128,
    return_probs=False,
    **kwargs,
):
    prompt = prompter.generate_prompt(instruction, input)
    inputs = tokenizer(prompt, return_tensors="pt")
    input_ids = inputs["input_ids"].to("cuda")

    generation_config = GenerationConfig(
        temperature=temperature,
        top_p=top_p,
        num_beams=num_beams,
        do_sample=do_sample,
        **kwargs,
    )

    with torch.no_grad():
        generation_output = model.generate(
            input_ids=input_ids,
            generation_config=generation_config,
            return_dict_in_generate=True,
            output_scores=True,
            max_new_tokens=max_new_tokens,
        )

    input_length = inputs.input_ids.shape[1]
    generated_tokens = generation_output.sequences[:, input_length:]
    output = tokenizer.decode(generated_tokens[0])

    if return_probs:
        transition_scores = model.compute_transition_scores(
            generation_output.sequences, generation_output.scores, normalize_logits=True
        )
        prob_dicts = []
        for tok, score in zip(generated_tokens[0], transition_scores[0]):
            prob_dicts.append({tokenizer.decode(tok):score.cpu().tolist()})

        return output, prob_dicts

    else:
        return output

env = wikienv.WikiEnv()
env = wrappers.MMLUWrapper(env, split="test")
env = wrappers.LoggingWrapper(env)

prompt_file = './prompts/prompts.json'
with open(prompt_file, 'r') as f:
    prompt_dict = json.load(f)

# standard prompt
mmlu_standard_examples = prompt_dict['mmlu_standard']
instruction_standard = """Answer the question:\n"""

# cot prompt
mmlu_cot_examples = prompt_dict['mmlu_cot']
instruction_cot = """Solve a question answering task. Your task is to generate Thought and Answer where a Thought can reason about the current situation by thinking step by step.
Here are some examples.
"""

# react prompt
mmlu_react_examples = prompt_dict['mmlu_react_google']
instruction_react_google = """Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation, and Action can be two types: 
(1) search[question], which searches a question on Google and returns a short snippet containing the answer. Note that sometimes the snippet does not contain the answer, and some alternative search might be needed.
(2) finish[answer], which returns the answer and finishes the task. 
Here are some examples.
"""

def standard(idx=None, instruction=instruction_standard, prompt=mmlu_standard_examples, to_print=True):
    question = env.reset(idx=idx)
    if to_print:
        print(idx, question)
    prompt += question + "\n"
    answer, probs = llama2_prompt(instruction, prompt + "Answer:", return_probs=True)
    answer = answer.split("\n")[0].strip()
    if to_print:
        print("Answer:", answer)

    token_probs = []
    for d in probs:
        d = {k.replace('<0x0A>', '\n'): v for k, v in d.items()}
        key = [key for key in d.keys()][0]
        if key in answer:
            token_probs.append({key:d[key]})
        else:
            break
    return answer, token_probs

def cot(idx=None, instruction=instruction_cot, prompt=mmlu_cot_examples, to_print=True):
    question = env.reset(idx=idx)
    if to_print:
        print(idx, question)
    prompt += question + "\n"
    answer, probs = llama2_prompt(instruction, prompt + "Thought:", return_probs=True)
    answer = answer.split("\nQuestion:")[0].strip()
    if to_print:
        print("Thought:", answer)

    token_probs = []
    for d in probs:
        d = {k.replace('<0x0A>', '\n'): v for k, v in d.items()}
        key = [key for key in d.keys()][0]
        if key in answer:
            token_probs.append({key:d[key]})
        else:
            break
    return answer, token_probs

def react(idx=None, instruction=instruction_react_google, prompt=mmlu_react_examples, to_print=True):
    question = env.reset(idx=idx)
    if to_print:
        print(idx, question)
    prompt += question + '\n'
    n_calls, n_badcalls = 0, 0
    steps = 0
    answer = ""
    react_probs = []
    for i in range(1, 8):
        n_calls += 1
        thought_action, thought_action_probs  = llama2_prompt(instruction, prompt + f"Thought:", return_probs=True)
        react_probs.append(thought_action_probs)
        try:
            thought = thought_action.strip().split(f"\nAction: ")[0]
            action = thought_action.strip().split(f"\nAction: ")[1].split("\n")[0]
        except:
            print('ohh...', thought_action)
            n_badcalls += 1
            n_calls += 1
            thought = thought_action.strip().split('\n')[0]
            action, action_probs = llama2_prompt(instruction, prompt + f"Thought: {thought}\nAction:", return_probs=True)
            react_probs.append(action_probs)
            action = action.split("\n")[0].strip()

        if action.startswith("search[") and action.endswith("]"):
            search_query = action[len("search["):-1]
            # tool calling
            obs = search(search_query)
            step_str = f"Thought: {thought}\nAction: {action}\nObservation: {obs}\n"
            if to_print:
                print(step_str)
            prompt += step_str
            steps = i
        elif action.startswith("finish[") and action.endswith("]"):
            answer = action[len("finish["):-1]
            step_str = f"Thought: {thought}\nAction: {action}\n"
            if to_print:
                print(step_str)
            prompt += step_str
            steps = i
            break
        else:
            print("Invalid action: {}".format(action))
            step_str = f"Thought: {thought}\nAction: finish[]\n"
            if to_print:
                print(step_str)
            prompt += step_str
            steps = i
            break
        
    info = {'steps':steps, "answer": answer, "gt_answer": gold_answer, "question_idx": idx, "reward": em, "em": em, 'n_calls': n_calls, 'n_badcalls': n_badcalls, 'traj': prompt}
    return info, react_probs


evals = []
old_time = time.time()

num_tool_call_instance = 0
num_instance = 0
num_correct = 0
num_tool_calls = 0
num_backoff = 0
num_ask_human = 0

with open(save_file_name,"a") as output_file:
    for i in tqdm(range(len(env))):
        question = env.reset(idx=i)
        gold_answer = env.data[i][1]
        num_instance += 1

        if mode == "standard":
            predicted_answer, _ = standard(i, to_print=True)
            print('-----------')
            em = (predicted_answer.lower() == gold_answer.lower())
            standard_final_output = {"answer": predicted_answer, "gt_answer": gold_answer, "question_idx": i, "em": em, "traj": question + '\nAnswer:' + predicted_answer}
            if standard_final_output["em"]:
                num_correct += 1
            output_file.write(json.dumps(standard_final_output, ensure_ascii=False) + '\n')

        elif mode == "cot":
            cot_output, _ = cot(i, to_print=True)
            print('-----------')
            try:
                predicted_answer = cot_output.split('Answer:')[1].strip()
            except:
                try:
                    predicted_answer = cot_output.split("the answer is ")[1].strip()
                except:
                    predicted_answer = ""
            em = (predicted_answer.lower() == gold_answer.lower())
            cot_final_output = {"answer": predicted_answer, "gt_answer": gold_answer, "question_idx": i, "em": em, "traj": question + '\nThought:' + cot_output}
            if cot_final_output["em"]:
                num_correct += 1
            output_file.write(json.dumps(cot_final_output, ensure_ascii=False) + '\n')

        elif mode == "react":
            info, _ = react(i, to_print=True)
            evals.append(info['em'])
            print(sum(evals), len(evals), sum(evals) / len(evals), (time.time() - old_time) / len(evals))
            print('-----------')
            info["traj"] = info["traj"].split(mmlu_react_examples)[1].strip()
            num_tool_calls += info["n_calls"]
            if info["em"]:
                num_correct += 1
            output_file.write(json.dumps(info, ensure_ascii=False) + '\n')

        elif mode == "uala":
            print('-----------Standard-----------')
            predicted_answer, probs = standard(i, to_print=True)

            em = (predicted_answer.lower() == gold_answer.lower())
            standard_final_output = {"steps": 2, "answer": predicted_answer, "gt_answer": gold_answer, "question_idx": i, "reward": em, "em": em, "n_calls": 0, "n_badcalls": 0, "traj": question + '\nThought:' + predicted_answer + f'\nAction: MeasureUncertainty [{predicted_answer}]'}
            
            # extract answer token logprobs for mmlu
            answer_probs = []
            for d in probs:
                for value in d.values():
                    answer_probs.append(value)

            # calculate uncertainty
            uncertainty = cal_uncertainty_single_token(answer_probs)

            # make tool use
            if uncertainty > uncertainty_threshold:
                print(f'-----------Answer’s uncertainty {round(uncertainty,2)}, which falls outside the acceptable uncertainty threshold of {uncertainty_threshold}, I need to use an external tool to solve the question.-----------')
                num_tool_call_instance += 1
                print('-----------ReAct-----------')
                info, react_probs = react(i, to_print=True)
                predicted_react_answer = info["answer"]
                standard_final_output["traj"] += f"\nObservation: Answer’s uncertainty is {round(uncertainty,2)}, which falls outside the acceptable threshold of {uncertainty_threshold}.\nThought: Based on the uncertainty, I need to use an external tool to solve the question.\nAction: Activate Tool.\n"
                
                def reformat(text):
                    return f"{text.group(1)} {int(text.group(2)) + 2}"

                react_traj = info["traj"].split(mmlu_react_examples + question + "\n")[1].strip()

                info["traj"] = standard_final_output["traj"] + react_traj 
                info["steps"] += standard_final_output["steps"]
                num_tool_calls += info["n_calls"]

                if info["answer"] in ['A', 'B', 'C', 'D']:
                    info["traj"] += f"\nThought: Let me check the uncertainty of the returned answer.\nAction: MeasureUncertainty [{predicted_react_answer}]"
                    info["steps"] += 1

                    # extract the answer token probs
                    answer_probs = []
                    for list in react_probs:
                        for key in list:
                            finish = False
                            if "finish" in key:
                                answer_probs = []
                                idx = list.index(key)
                                for i in range(idx+1,len(list)):
                                    for key, value in list[i].items():
                                        if key == '[':
                                            continue
                                        if key == ']':
                                            finish = True
                                            break
                                        answer_probs.append(value)
                                    if finish:
                                        break
                    if not answer_probs:
                        answer_probs = [0.0]

                    react_uncertainty = cal_uncertainty_single_token(answer_probs)
                    if react_uncertainty > uncertainty_threshold:
                        info["steps"] += 1
                        if oracle:
                            print(f"-----------Answer’s uncertainty is {round(react_uncertainty,2)}, which falls outside the acceptable threshold of {uncertainty_threshold}, ask a human for help.-----------")
                            info["traj"] += f"\nObservation: Answer’s uncertainty is {round(react_uncertainty,2)}, which falls outside the acceptable threshold of {uncertainty_threshold}.\nThought: Based on the uncertainty, I need to ask a human for help.\nAction: Ask Human.\nObservation: {gold_answer}\nAnswer: {gold_answer}"
                            info["answer"] = gold_answer
                            info["reward"] = True
                            info["em"] = True
                            num_ask_human += 1
                        else:
                            print(f"-----------Answer’s uncertainty is {round(react_uncertainty,2)}, which falls outside the acceptable threshold of {uncertainty_threshold}, for simplicity, answer is still kept.-----------")
                            info["traj"] += f"\nObservation: Answer’s uncertainty is {round(react_uncertainty,2)}, which falls outside the acceptable threshold of {uncertainty_threshold}.\nThought: For simplicity, answer is still kept.\nAction: Keep Answer.\nAnswer: {predicted_react_answer}"
                            
                    else:
                        print(f"-----------Answer’s uncertainty is {round(react_uncertainty,2)}, which falls within the acceptable threshold of {uncertainty_threshold}, answer is kept.-----------")
                        info["traj"] += f"\nObservation: Answer’s uncertainty is {round(react_uncertainty,2)}, which falls within the acceptable threshold of {uncertainty_threshold}.\nThought: Based on the uncertainty, answer is kept.\nAction: Keep Answer.\nAnswer: {predicted_react_answer}"
                        info["steps"] += 1
                    
                else:
                    print("-----------Returned tool-use answer is invalid, I need to use the backoff answer.-----------")
                    use_backoff_traj = f"\nThought: Returned tool-use answer is invalid, I need to use the backoff answer.\nAction: Use Backoff Answer.\nAnswer: {predicted_answer}"
                    info["traj"] += use_backoff_traj
                    num_backoff += 1
                    info["answer"] = standard_final_output["answer"]
                    info["reward"] = standard_final_output["reward"]
                    info["em"] = standard_final_output["em"]

                if info["em"]:
                    num_correct += 1
                output_file.write(json.dumps(info, ensure_ascii=False) + '\n')
            else:
                print(f'-----------Answer’s uncertainty is {round(uncertainty,2)}, which falls within the acceptable threshold of {uncertainty_threshold}, answer is kept.-----------')
                standard_final_output["traj"] += f"\nObservation 1: Answer’s uncertainty is {round(uncertainty,2)}, which falls within the acceptable threshold of {uncertainty_threshold}.\nThought: Based on the uncertainty, answer is kept.\nAction: Keep Answer.\nAnswer: {predicted_answer}"
                if standard_final_output["em"]:
                    num_correct += 1
                output_file.write(json.dumps(standard_final_output, ensure_ascii=False) + '\n')
